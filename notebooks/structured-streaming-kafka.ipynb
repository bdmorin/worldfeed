{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "# Since we know the data format already, let's define the schema to speed up processing (no need for Spark to infer schema)\n",
    "jsonSchema = StructType([ StructField(\"last_reported\", TimestampType(), True), \n",
    "                          StructField(\"name\", StringType(), True),\n",
    "                          StructField(\"station_id\", StringType(), True),\n",
    "                          StructField(\"region_id\", StringType(), True),\n",
    "                          StructField(\"publisher\", StringType(), True),\n",
    "                          StructField(\"lat\", FloatType(), True),\n",
    "                          StructField(\"lon\", FloatType(), True),\n",
    "                          StructField(\"country_code\", StringType(), True),\n",
    "                          StructField(\"num_bikes_available\", IntegerType(), True),\n",
    "                          StructField(\"num_docks_available\", IntegerType(), True),\n",
    "                          StructField(\"is_renting\", IntegerType(), True),\n",
    "                          StructField(\"is_returning\", IntegerType(), True)\n",
    "                        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stream Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Similar to definition of staticInputDF above, just using `readStream` instead of `read`\n",
    "parsed = (\n",
    "  spark\n",
    "    .readStream                       \n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\n",
    "    .option(\"subscribe\", \"satori-bike\")\n",
    "    .load()\n",
    "    .select(col(\"timestamp\"),from_json(col(\"value\").cast(\"string\"),jsonSchema).alias(\"parsed_value\"))\n",
    ")\n",
    "\n",
    "bikedata = parsed.select(\"timestamp\",\"parsed_value.*\")\n",
    "bikedata.printSchema()\n",
    "bikedata.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")  # keep the size of shuffles small\n",
    "#query.stop()\n",
    "query = (\n",
    "  bikedata\n",
    "    .withWatermark(\"timestamp\", \"10 minutes\")\n",
    "    .groupBy(\n",
    "       bikedata.country_code, \n",
    "       window(bikedata.last_reported, \"10 minutes\", \"5 minutes\"))    \n",
    "    .count()\n",
    "    .writeStream\n",
    "    .format(\"memory\")        # memory = store in-memory table (for testing only in Spark 2.0)\n",
    "    .outputMode(\"complete\")  # complete = all the counts should be in the table\n",
    "    .queryName(\"bikesharing\")     # counts = name of the in-memory table\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spark.sql(\"select * from bikesharing\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spark.sql(\"select country_code, date_format(window.start, 'MMM-dd HH:mm') as time_start, date_format(window.end, 'MMM-dd HH:mm') as time_end, count from bikesharing order by time_end, country_code\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, let's see the total number of \"opens\" and \"closes\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spark.sql(\"select country_code, sum(count) as total_count from bikesharing group by country_code order by country_code\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# RUN a simple webservice on default port 5000 to return the in-memory table as is\n",
    "from flask import Flask\n",
    "import json\n",
    "app = Flask(__name__)\n",
    "\n",
    "# route to access data\n",
    "@app.route(\"/\")\n",
    "def count_per_country():\n",
    "    counts = spark.sql(\"select country_code, sum(count) as total_count from bikesharing group by country_code order by country_code\").toJSON().collect()\n",
    "    return json.dumps(counts)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(query.lastProgress)\n",
    "\n",
    "# check for running streams:\n",
    "#spark.streams.active\n",
    "\n",
    "#Terminate the query stream\n",
    "#query.stop()\n",
    "# some stats for the query\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "PySpark (Python 3, Spark 2.1.1)",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "name": "structured-streaming-python",
  "notebookId": 1458106420266562
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
