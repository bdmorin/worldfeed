{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "# Since we know the data format already, let's define the schema to speed up processing (no need for Spark to infer schema)\n",
    "jsonSchema = StructType([ StructField(\"last_reported\", TimestampType(), True), \n",
    "                          StructField(\"name\", StringType(), True),\n",
    "                          StructField(\"station_id\", StringType(), True),\n",
    "#                          StructField(\"region_id\", StringType(), True),\n",
    "#                          StructField(\"publisher\", StringType(), True),\n",
    "                          StructField(\"lat\", FloatType(), True),\n",
    "                          StructField(\"lon\", FloatType(), True),\n",
    "                          StructField(\"country_code\", StringType(), True),\n",
    "#                          StructField(\"num_bikes_available\", IntegerType(), True),\n",
    "#                          StructField(\"num_docks_available\", IntegerType(), True)\n",
    "#                          StructField(\"is_renting\", IntegerType(), True),\n",
    "#                          StructField(\"is_returning\", IntegerType(), True)\n",
    "                        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stream Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Similar to definition of staticInputDF above, just using `readStream` instead of `read`\n",
    "parsed = (\n",
    "  spark\n",
    "    .readStream                       \n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\n",
    "    .option(\"subscribe\", \"satori-bike\")\n",
    "    .load()\n",
    "    .select(col(\"timestamp\"),from_json(col(\"value\").cast(\"string\"),jsonSchema).alias(\"parsed_value\"))\n",
    ")\n",
    "\n",
    "bikedata = parsed.select(\"timestamp\",\"parsed_value.*\")\n",
    "bikedata.printSchema()\n",
    "bikedata.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")  # keep the size of shuffles small\n",
    "#query.stop()\n",
    "#query = (\n",
    "#  bikedata\n",
    "#    .withWatermark(\"timestamp\", \"1 minutes\")\n",
    "#    .groupBy(\n",
    "#       bikedata.country_code, \n",
    "#       window(bikedata.last_reported, \"1 minutes\", \"30 seconds\"))  \n",
    "#    .count()\n",
    "#    .select(to_json(struct(\"window\")).alias(\"key\"),\n",
    "#            to_json(struct(\"country_code\",\"count\")).alias(\"value\"))\n",
    "#    .writeStream\n",
    "#    .trigger(processingTime='10 seconds') # only write every 10 seconds to the output\n",
    "#    .format(\"console\")        # memory = store in-memory table (for testing only in Spark 2.0)\n",
    "#    .outputMode(\"update\")  # complete = all the counts should be in the table\n",
    "#    .queryName(\"bikesharing\")     # counts = name of the in-memory table\n",
    "#    .start()\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")  # keep the size of shuffles small\n",
    "\n",
    "query = (\n",
    "  bikedata\n",
    "    .withWatermark(\"timestamp\", \"1 hour\")\n",
    "    .groupBy(\n",
    "       bikedata.country_code, \n",
    "       window(bikedata.last_reported, \"1 hour\", \"30 minutes\"))  \n",
    "    .count()\n",
    "    .select(to_json(struct(\"country_code\", \"window\")).alias(\"key\"),\n",
    "            to_json(struct(\"window.start\",\"window.end\",\"country_code\",\"count\")).alias(\"value\"))\n",
    "#        col(\"count\").cast(\"string\").alias(\"value\"))\n",
    "    .writeStream\n",
    "    .trigger(processingTime='2 seconds')\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\n",
    "    .option(\"topic\", \"aggr-bike\")\n",
    "    .option(\"checkpointLocation\", \"./checkpoints\")\n",
    "    .outputMode(\"update\")  # complete = all the counts should be in the table\n",
    "    .queryName(\"bikesharing\")     # counts = name of the in-memory table\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(query.lastProgress)\n",
    "\n",
    "# check for running streams:\n",
    "#spark.streams.active\n",
    "\n",
    "#Terminate the query stream\n",
    "#query.stop()\n",
    "# some stats for the query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "PySpark (Python 3, Spark 2.1.1)",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "name": "structured-streaming-python",
  "notebookId": 1458106420266562
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
