{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_0.json    file_126.json  file_16.json   file_44.json   file_72.json\r\n",
      "file_1.json    file_127.json  file_17.json   file_45.json   file_73.json\r\n",
      "file_10.json   file_128.json  file_18.json   file_46.json   file_74.json\r\n",
      "file_100.json  file_129.json  file_19.json   file_47.json   file_75.json\r\n",
      "file_101.json  file_13.json   file_2.json    file_48.json   file_76.json\r\n",
      "file_102.json  file_130.json  file_20.json   file_49.json   file_77.json\r\n",
      "file_103.json  file_131.json  file_21.json   file_5.json    file_78.json\r\n",
      "file_104.json  file_132.json  file_22.json   file_50.json   file_79.json\r\n",
      "file_105.json  file_133.json  file_23.json   file_51.json   file_8.json\r\n",
      "file_106.json  file_134.json  file_24.json   file_52.json   file_80.json\r\n",
      "file_107.json  file_135.json  file_25.json   file_53.json   file_81.json\r\n",
      "file_108.json  file_136.json  file_26.json   file_54.json   file_82.json\r\n",
      "file_109.json  file_137.json  file_27.json   file_55.json   file_83.json\r\n",
      "file_11.json   file_138.json  file_28.json   file_56.json   file_84.json\r\n",
      "file_110.json  file_139.json  file_29.json   file_57.json   file_85.json\r\n",
      "file_111.json  file_14.json   file_3.json    file_58.json   file_86.json\r\n",
      "file_112.json  file_140.json  file_30.json   file_59.json   file_87.json\r\n",
      "file_113.json  file_141.json  file_31.json   file_6.json    file_88.json\r\n",
      "file_114.json  file_142.json  file_32.json   file_60.json   file_89.json\r\n",
      "file_115.json  file_143.json  file_33.json   file_61.json   file_9.json\r\n",
      "file_116.json  file_144.json  file_34.json   file_62.json   file_90.json\r\n",
      "file_117.json  file_145.json  file_35.json   file_63.json   file_91.json\r\n",
      "file_118.json  file_146.json  file_36.json   file_64.json   file_92.json\r\n",
      "file_119.json  file_147.json  file_37.json   file_65.json   file_93.json\r\n",
      "file_12.json   file_148.json  file_38.json   file_66.json   file_94.json\r\n",
      "file_120.json  file_149.json  file_39.json   file_67.json   file_95.json\r\n",
      "file_121.json  file_15.json   file_4.json    file_68.json   file_96.json\r\n",
      "file_122.json  file_150.json  file_40.json   file_69.json   file_97.json\r\n",
      "file_123.json  file_151.json  file_41.json   file_7.json    file_98.json\r\n",
      "file_124.json  file_152.json  file_42.json   file_70.json   file_99.json\r\n",
      "file_125.json  file_153.json  file_43.json   file_71.json\r\n"
     ]
    }
   ],
   "source": [
    "ls ./data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are about 50 JSON files in the directory. Let's see what each JSON file contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"name\": \"BR 7\", \"region_id\": \"region_107\", \"lat\": 37.771557513543236, \"station_id\": \"hub_516\", \"channel\": \"US-Bike-Sharing-Channel\", \"num_bikes_available\": 3, \"country_code\": \"US\", \"last_reported\": 1495379589, \"is_returning\": 1, \"rental_methods\": [\"KEY\", \"APPLEPAY\", \"ANDROIDPAY\", \"TRANSITCARD\", \"ACCOUNTNUMBER\", \"PHONE\"], \"is_installed\": 1, \"lon\": -121.96133270859717, \"num_bikes_disabled\": 0, \"is_renting\": 1, \"address\": \"USPS, San Ramon Valley Iron Horse Trail, San Ramon, California\", \"num_docks_available\": 3, \"publisher\": \"britebikes.socialbicycles.com\"}\r\n",
      "{\"name\": \"San Ramon Test\", \"region_id\": \"region_122\", \"lat\": 37.7714624826912, \"station_id\": \"hub_662\", \"channel\": \"US-Bike-Sharing-Channel\", \"num_bikes_available\": 2, \"country_code\": \"US\", \"last_reported\": 1495379589, \"is_returning\": 1, \"rental_methods\": [\"KEY\", \"APPLEPAY\", \"ANDROIDPAY\", \"TRANSITCARD\", \"ACCOUNTNUMBER\", \"PHONE\"], \"is_installed\": 1, \"lon\": -121.992872431289, \"num_bikes_disabled\": 0, \"is_renting\": 1, \"address\": \"2401 Crow Canyon Road, San Ramon\", \"num_docks_available\": 2, \"publisher\": \"britebikes.socialbicycles.com\"}\r\n",
      "{\"name\": \"San Ramon City Hall\", \"region_id\": \"region_107\", \"lat\": 37.76444079795254, \"station_id\": \"hub_1458\", \"channel\": \"US-Bike-Sharing-Channel\", \"num_bikes_available\": 2, \"country_code\": \"US\", \"last_reported\": 1495379589, \"is_returning\": 1, \"rental_methods\": [\"KEY\", \"APPLEPAY\", \"ANDROIDPAY\", \"TRANSITCARD\", \"ACCOUNTNUMBER\", \"PHONE\"], \"is_installed\": 1, \"lon\": -121.95507645606993, \"num_bikes_disabled\": 0, \"is_renting\": 1, \"address\": \"Iron Horse Regional Trail, San Ramon\", \"num_docks_available\": 2, \"publisher\": \"britebikes.socialbicycles.com\"}\r\n",
      "{\"name\": \"BR 2600 Visitor Lot\", \"region_id\": \"region_107\", \"lat\": 37.766851630534205, \"station_id\": \"hub_1461\", \"channel\": \"US-Bike-Sharing-Channel\", \"num_bikes_available\": 8, \"country_code\": \"US\", \"last_reported\": 1495379589, \"is_returning\": 1, \"rental_methods\": [\"KEY\", \"APPLEPAY\", \"ANDROIDPAY\", \"TRANSITCARD\", \"ACCOUNTNUMBER\", \"PHONE\"], \"is_installed\": 1, \"lon\": -121.96319282054901, \"num_bikes_disabled\": 0, \"is_renting\": 1, \"address\": \"2600 Camino Ramon, San Ramon\", \"num_docks_available\": 2, \"publisher\": \"britebikes.socialbicycles.com\"}\r\n",
      "{\"name\": \"BR 3 North\", \"region_id\": \"region_107\", \"lat\": 37.76856481823885, \"station_id\": \"hub_1732\", \"channel\": \"US-Bike-Sharing-Channel\", \"num_bikes_available\": 6, \"country_code\": \"US\", \"last_reported\": 1495379589, \"is_returning\": 1, \"rental_methods\": [\"KEY\", \"APPLEPAY\", \"ANDROIDPAY\", \"TRANSITCARD\", \"ACCOUNTNUMBER\", \"PHONE\"], \"is_installed\": 1, \"lon\": -121.95952892303467, \"num_bikes_disabled\": 0, \"is_renting\": 1, \"address\": \"2613 Camino Ramon, San Ramon\", \"num_docks_available\": 4, \"publisher\": \"britebikes.socialbicycles.com\"}\r\n",
      "{\"name\": \"BR 3 South\", \"region_id\": \"region_107\", \"lat\": 37.76695339964549, \"station_id\": \"hub_1733\", \"channel\": \"US-Bike-Sharing-Channel\", \"num_bikes_available\": 10, \"country_code\": \"US\", \"last_reported\": 1495379589, \"is_returning\": 1, \"rental_methods\": [\"KEY\", \"APPLEPAY\", \"ANDROIDPAY\", \"TRANSITCARD\", \"ACCOUNTNUMBER\", \"PHONE\"], \"is_installed\": 1, \"lon\": -121.95842921733856, \"num_bikes_disabled\": 0, \"is_renting\": 1, \"address\": \"2623 Camino Ramon, San Ramon\", \"num_docks_available\": 0, \"publisher\": \"britebikes.socialbicycles.com\"}\r\n",
      "{\"name\": \"BR 12 North\", \"region_id\": \"region_107\", \"lat\": 37.773450277034094, \"station_id\": \"hub_1985\", \"channel\": \"US-Bike-Sharing-Channel\", \"num_bikes_available\": 2, \"country_code\": \"US\", \"last_reported\": 1495379589, \"is_returning\": 1, \"rental_methods\": [\"KEY\", \"APPLEPAY\", \"ANDROIDPAY\", \"TRANSITCARD\", \"ACCOUNTNUMBER\", \"PHONE\"], \"is_installed\": 1, \"lon\": -121.9721420109272, \"num_bikes_disabled\": 0, \"is_renting\": 1, \"address\": \"Annabel Lane, Bishop Ranch Business Park, San Ramon, California\", \"num_docks_available\": 2, \"publisher\": \"britebikes.socialbicycles.com\"}\r\n",
      "{\"name\": \"BR 15 North\", \"region_id\": \"region_107\", \"lat\": 37.77477693179412, \"station_id\": \"hub_2005\", \"channel\": \"US-Bike-Sharing-Channel\", \"num_bikes_available\": 5, \"country_code\": \"US\", \"last_reported\": 1495379589, \"is_returning\": 1, \"rental_methods\": [\"KEY\", \"APPLEPAY\", \"ANDROIDPAY\", \"TRANSITCARD\", \"ACCOUNTNUMBER\", \"PHONE\"], \"is_installed\": 1, \"lon\": -121.96133136749268, \"num_bikes_disabled\": 0, \"is_renting\": 1, \"address\": \"USPS, San Ramon Valley Iron Horse Trail, San Ramon, California\", \"num_docks_available\": 5, \"publisher\": \"britebikes.socialbicycles.com\"}\r\n",
      "{\"name\": \"BR 15 South\", \"region_id\": \"region_107\", \"lat\": 37.77319109165642, \"station_id\": \"hub_2006\", \"channel\": \"US-Bike-Sharing-Channel\", \"num_bikes_available\": 3, \"country_code\": \"US\", \"last_reported\": 1495379589, \"is_returning\": 1, \"rental_methods\": [\"KEY\", \"APPLEPAY\", \"ANDROIDPAY\", \"TRANSITCARD\", \"ACCOUNTNUMBER\", \"PHONE\"], \"is_installed\": 1, \"lon\": -121.95986688137053, \"num_bikes_disabled\": 0, \"is_renting\": 1, \"address\": \"12657 Alcosta Boulevard, San Ramon\", \"num_docks_available\": 6, \"publisher\": \"britebikes.socialbicycles.com\"}\r\n",
      "{\"name\": \"BR 9\", \"region_id\": \"region_107\", \"lat\": 37.77424584757111, \"station_id\": \"hub_2007\", \"channel\": \"US-Bike-Sharing-Channel\", \"num_bikes_available\": 5, \"country_code\": \"US\", \"last_reported\": 1495379589, \"is_returning\": 1, \"rental_methods\": [\"KEY\", \"APPLEPAY\", \"ANDROIDPAY\", \"TRANSITCARD\", \"ACCOUNTNUMBER\", \"PHONE\"], \"is_installed\": 1, \"lon\": -121.96375474333762, \"num_bikes_disabled\": 0, \"is_renting\": 1, \"address\": \"Camino Ramon, Bishop Ranch Business Park, San Ramon, California\", \"num_docks_available\": 3, \"publisher\": \"britebikes.socialbicycles.com\"}\r\n"
     ]
    }
   ],
   "source": [
    "cat data/file_0.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "inputPath = \"./data/\"\n",
    "\n",
    "# Since we know the data format already, let's define the schema to speed up processing (no need for Spark to infer schema)\n",
    "jsonSchema = StructType([ StructField(\"last_reported\", TimestampType(), True), \n",
    "                          StructField(\"name\", StringType(), True),\n",
    "                          StructField(\"station_id\", StringType(), True),\n",
    "                          StructField(\"region_id\", StringType(), True),\n",
    "                          StructField(\"publisher\", StringType(), True),\n",
    "                          StructField(\"lat\", FloatType(), True),\n",
    "                          StructField(\"lon\", FloatType(), True),\n",
    "                          StructField(\"country_code\", StringType(), True),\n",
    "                          StructField(\"num_bikes_available\", IntegerType(), True),\n",
    "                          StructField(\"num_docks_available\", IntegerType(), True),\n",
    "                          StructField(\"is_renting\", IntegerType(), True),\n",
    "                          StructField(\"is_returning\", IntegerType(), True)\n",
    "                        ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Static Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1533"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Static DataFrame representing data in the JSON files\n",
    "staticInputDF = (\n",
    "  spark\n",
    "    .read\n",
    "    .schema(jsonSchema)\n",
    "    .json(inputPath)\n",
    ")\n",
    "\n",
    "#display(staticInputDF)\n",
    "staticInputDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compute the number of \"open\" and \"close\" actions with one hour windows. To do this, we will group by the `action` column and 1 hour windows over the `time` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *      # for window() function\n",
    "\n",
    "staticCountsperCountryDF = (\n",
    "  staticInputDF\n",
    "    .groupBy(\n",
    "       staticInputDF.country_code, \n",
    "       window(staticInputDF.last_reported, \"1 minute\"))    \n",
    "    .count()\n",
    ")\n",
    "staticCountsperCountryDF.cache()\n",
    "\n",
    "# Register the DataFrame as table 'static_counts'\n",
    "staticCountsperCountryDF.createOrReplaceTempView(\"country_counts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "|country_code|total_count|\n",
      "+------------+-----------+\n",
      "|          AU|         41|\n",
      "|          CA|        119|\n",
      "|          US|       1155|\n",
      "+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select country_code, sum(count) as total_count from country_counts group by country_code\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+-----+\n",
      "|country_code|              window|count|\n",
      "+------------+--------------------+-----+\n",
      "|          AU|[2017-05-21 17:13...|   41|\n",
      "+------------+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from country_counts where country_code = 'AU'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about a timeline of windowed counts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+-----+\n",
      "|country_code|        time|count|\n",
      "+------------+------------+-----+\n",
      "|          US|May-21 17:13|    3|\n",
      "|          AU|May-21 17:14|   41|\n",
      "|          CA|May-21 17:14|  119|\n",
      "|          US|May-21 17:14| 1152|\n",
      "+------------+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select country_code, date_format(window.end, 'MMM-dd HH:mm') as time, count from country_counts order by time, country_code\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the two ends of the graph. The close actions are generated such that they are after the corresponding open actions, so there are more \"opens\" in the beginning and more \"closes\" in the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stream Processing \n",
    "Now that we have analyzed the data interactively, let's convert this to a streaming query that continuously updates as data comes. Since we just have a static set of files, we are going to emulate a stream from them by reading one file at a time, in the chronological order they were created. The query we have to write is pretty much the same as the interactive query above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Similar to definition of staticInputDF above, just using `readStream` instead of `read`\n",
    "streamingInputDF = (\n",
    "  spark\n",
    "    .readStream                       \n",
    "    .schema(jsonSchema)               # Set the schema of the JSON data\n",
    "    .option(\"maxFilesPerTrigger\", 1)  # Treat a sequence of files as a stream by picking one file at a time\n",
    "    .json(inputPath)\n",
    ")\n",
    "\n",
    "# Same query as staticInputDF\n",
    "streamingCountsDF = (                 \n",
    "  streamingInputDF\n",
    "    .groupBy(\n",
    "      streamingInputDF.country_code, \n",
    "      window(streamingInputDF.last_reported, \"1 minute\"))\n",
    "    .count()\n",
    ")\n",
    "\n",
    "# Is this DF actually a streaming DF?\n",
    "streamingCountsDF.isStreaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, `streamingCountsDF` is a streaming Dataframe (`streamingCountsDF.isStreaming` was `true`). You can start streaming computation, by defining the sink and starting it. \n",
    "In our case, we want to interactively query the counts (same queries as above), so we will set the complete set of 1 hour counts to be in a in-memory table (note that this for testing purpose only in Spark 2.0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")  # keep the size of shuffles small\n",
    "\n",
    "query = (\n",
    "  streamingCountsDF\n",
    "    .writeStream\n",
    "    .format(\"memory\")        # memory = store in-memory table (for testing only in Spark 2.0)\n",
    "    .queryName(\"counts\")     # counts = name of the in-memory table\n",
    "    .outputMode(\"complete\")  # complete = all the counts should be in the table\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`query` is a handle to the streaming query that is running in the background. This query is continuously picking up files and updating the windowed counts. \n",
    "\n",
    "Note the status of query in the above cell. Both the `Status: ACTIVE` and the progress bar shows that the query is active. \n",
    "Furthermore, if you expand the `>Details` above, you will find the number of files they have already processed. \n",
    "\n",
    "Let's wait a bit for a few files to be processed and then interactively query the in-memory `counts` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "sleep(2)  # wait a bit for computation to start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+-----+\n",
      "|country_code|        time|count|\n",
      "+------------+------------+-----+\n",
      "|          US|May-21 17:14|   40|\n",
      "+------------+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select country_code, date_format(window.end, 'MMM-dd HH:mm') as time, count from counts order by time, country_code\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the timeline of windowed counts (similar to the static one ealrier) building up. If we keep running this interactive query repeatedly, we will see the latest updated counts which the streaming query is updating in the background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sleep(2)  # wait a bit more for more data to be computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+-----+\n",
      "|country_code|        time|count|\n",
      "+------------+------------+-----+\n",
      "|          US|May-21 17:14|  139|\n",
      "+------------+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sleep(2)  # wait a bit more for more data to be computed\n",
    "spark.sql(\"select country_code, date_format(window.end, 'MMM-dd HH:mm') as time, count from counts order by time, country_code\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, let's see the total number of \"opens\" and \"closes\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(country_code='US', total_count=149)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select country_code, sum(count) as total_count from counts group by country_code order by country_code\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [21/May/2017 17:47:00] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [21/May/2017 17:47:01] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [21/May/2017 17:47:02] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [21/May/2017 17:47:03] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [21/May/2017 17:47:03] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [21/May/2017 17:47:04] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [21/May/2017 17:47:04] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [21/May/2017 17:47:05] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [21/May/2017 17:47:05] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [21/May/2017 17:47:05] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [21/May/2017 17:47:05] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [21/May/2017 17:47:05] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [21/May/2017 17:47:06] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [21/May/2017 17:47:06] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [21/May/2017 17:47:06] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [21/May/2017 17:47:06] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [21/May/2017 17:47:06] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [21/May/2017 17:47:07] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [21/May/2017 17:47:07] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [21/May/2017 17:47:07] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [21/May/2017 17:47:07] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [21/May/2017 17:47:08] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [21/May/2017 17:47:08] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [21/May/2017 17:47:08] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [21/May/2017 17:47:08] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [21/May/2017 17:47:09] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [21/May/2017 17:47:09] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [21/May/2017 17:47:09] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [21/May/2017 17:47:09] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [21/May/2017 17:47:09] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [21/May/2017 17:47:10] \"GET / HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 53613)\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/mca/anaconda/envs/python3/lib/python3.5/socketserver.py\", line 313, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/Users/mca/anaconda/envs/python3/lib/python3.5/socketserver.py\", line 341, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/Users/mca/anaconda/envs/python3/lib/python3.5/socketserver.py\", line 354, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/Users/mca/anaconda/envs/python3/lib/python3.5/socketserver.py\", line 681, in __init__\n",
      "    self.handle()\n",
      "  File \"/Users/mca/anaconda/envs/python3/lib/python3.5/site-packages/werkzeug/serving.py\", line 232, in handle\n",
      "    rv = BaseHTTPRequestHandler.handle(self)\n",
      "  File \"/Users/mca/anaconda/envs/python3/lib/python3.5/http/server.py\", line 422, in handle\n",
      "    self.handle_one_request()\n",
      "  File \"/Users/mca/anaconda/envs/python3/lib/python3.5/site-packages/werkzeug/serving.py\", line 263, in handle_one_request\n",
      "    self.raw_requestline = self.rfile.readline()\n",
      "  File \"/Users/mca/anaconda/envs/python3/lib/python3.5/socket.py\", line 576, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# RUN a simple webservice on default port 5000 to return the in-memory table as is\n",
    "from flask import Flask\n",
    "import json\n",
    "app = Flask(__name__)\n",
    "\n",
    "# route to access data\n",
    "@app.route(\"/\")\n",
    "def count_per_country():\n",
    "    counts = spark.sql(\"select country_code, sum(count) as total_count from counts group by country_code\").toJSON().collect()\n",
    "    return json.dumps(counts)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "PySpark (Python 3, Spark 2.1.0)",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "name": "structured-streaming-python",
  "notebookId": 1458106420266562
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
