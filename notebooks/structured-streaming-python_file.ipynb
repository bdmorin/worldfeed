{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ls ./data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are about 50 JSON files in the directory. Let's see what each JSON file contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cat data/file_0.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "inputPath = \"./data/\"\n",
    "\n",
    "# Since we know the data format already, let's define the schema to speed up processing (no need for Spark to infer schema)\n",
    "jsonSchema = StructType([ StructField(\"last_reported\", TimestampType(), True), \n",
    "                          StructField(\"name\", StringType(), True),\n",
    "                          StructField(\"station_id\", StringType(), True),\n",
    "                          StructField(\"region_id\", StringType(), True),\n",
    "                          StructField(\"publisher\", StringType(), True),\n",
    "                          StructField(\"lat\", FloatType(), True),\n",
    "                          StructField(\"lon\", FloatType(), True),\n",
    "                          StructField(\"country_code\", StringType(), True),\n",
    "                          StructField(\"num_bikes_available\", IntegerType(), True),\n",
    "                          StructField(\"num_docks_available\", IntegerType(), True),\n",
    "                          StructField(\"is_renting\", IntegerType(), True),\n",
    "                          StructField(\"is_returning\", IntegerType(), True)\n",
    "                        ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Static Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Static DataFrame representing data in the JSON files\n",
    "staticInputDF = (\n",
    "  spark\n",
    "    .read\n",
    "    .schema(jsonSchema)\n",
    "    .json(inputPath)\n",
    ")\n",
    "\n",
    "#display(staticInputDF)\n",
    "staticInputDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compute the number of \"open\" and \"close\" actions with one hour windows. To do this, we will group by the `action` column and 1 hour windows over the `time` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *      # for window() function\n",
    "\n",
    "staticCountsperCountryDF = (\n",
    "  staticInputDF\n",
    "    .groupBy(\n",
    "       staticInputDF.country_code, \n",
    "       window(staticInputDF.last_reported, \"1 minute\"))    \n",
    "    .count()\n",
    ")\n",
    "staticCountsperCountryDF.cache()\n",
    "\n",
    "# Register the DataFrame as table 'static_counts'\n",
    "staticCountsperCountryDF.createOrReplaceTempView(\"country_counts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark.sql(\"select country_code, sum(count) as total_count from country_counts group by country_code\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spark.sql(\"select * from country_counts where country_code = 'AU'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about a timeline of windowed counts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spark.sql(\"select country_code, date_format(window.end, 'MMM-dd HH:mm') as time, count from country_counts order by time, country_code\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the two ends of the graph. The close actions are generated such that they are after the corresponding open actions, so there are more \"opens\" in the beginning and more \"closes\" in the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stream Processing \n",
    "Now that we have analyzed the data interactively, let's convert this to a streaming query that continuously updates as data comes. Since we just have a static set of files, we are going to emulate a stream from them by reading one file at a time, in the chronological order they were created. The query we have to write is pretty much the same as the interactive query above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Similar to definition of staticInputDF above, just using `readStream` instead of `read`\n",
    "streamingInputDF = (\n",
    "  spark\n",
    "    .readStream                       \n",
    "    .schema(jsonSchema)               # Set the schema of the JSON data\n",
    "    .option(\"maxFilesPerTrigger\", 1)  # Treat a sequence of files as a stream by picking one file at a time\n",
    "    .json(inputPath)\n",
    ")\n",
    "\n",
    "# Same query as staticInputDF\n",
    "streamingCountsDF = (                 \n",
    "  streamingInputDF\n",
    "    .groupBy(\n",
    "      streamingInputDF.country_code, \n",
    "      window(streamingInputDF.last_reported, \"1 minute\"))\n",
    "    .count()\n",
    ")\n",
    "\n",
    "# Is this DF actually a streaming DF?\n",
    "streamingCountsDF.isStreaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, `streamingCountsDF` is a streaming Dataframe (`streamingCountsDF.isStreaming` was `true`). You can start streaming computation, by defining the sink and starting it. \n",
    "In our case, we want to interactively query the counts (same queries as above), so we will set the complete set of 1 hour counts to be in a in-memory table (note that this for testing purpose only in Spark 2.0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")  # keep the size of shuffles small\n",
    "\n",
    "query = (\n",
    "  streamingCountsDF\n",
    "    .writeStream\n",
    "    .format(\"memory\")        # memory = store in-memory table (for testing only in Spark 2.0)\n",
    "    .outputMode(\"complete\")  # complete = all the counts should be in the table\n",
    "    .queryName(\"counts\")     # counts = name of the in-memory table\n",
    "    .option(\"checkpointLocation\", \"./checkpoints\")\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# write to file in append mode\n",
    "#query = (\n",
    "#  streamingCountsDF\n",
    "#    .writeStream\n",
    "#    .queryName(\"counts\")\n",
    "#    .format(\"parquet\") \n",
    "#    .outputMode(\"update\")\n",
    "#    .option(\"path\", \"./outdir\")\n",
    "#    .option(\"checkpointLocation\", \"./checkpoints\")\n",
    "#    .start()\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`query` is a handle to the streaming query that is running in the background. This query is continuously picking up files and updating the windowed counts. \n",
    "\n",
    "Note the status of query in the above cell. Both the `Status: ACTIVE` and the progress bar shows that the query is active. \n",
    "Furthermore, if you expand the `>Details` above, you will find the number of files they have already processed. \n",
    "\n",
    "Let's wait a bit for a few files to be processed and then interactively query the in-memory `counts` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# RUN a simple webservice on default port 5000 to return the in-memory table as is\n",
    "from flask import Flask\n",
    "import json\n",
    "app = Flask(__name__)\n",
    "\n",
    "# route to access data\n",
    "@app.route(\"/\")\n",
    "def count_per_country():\n",
    "    counts = spark.sql(\"select country_code, sum(count) as total_count from counts group by country_code\").toJSON().collect()\n",
    "    return json.dumps(counts)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "sleep(2)  # wait a bit for computation to start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spark.sql(\"select country_code, date_format(window.end, 'MMM-dd HH:mm') as time, count from counts order by time, country_code\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the timeline of windowed counts (similar to the static one ealrier) building up. If we keep running this interactive query repeatedly, we will see the latest updated counts which the streaming query is updating in the background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sleep(2)  # wait a bit more for more data to be computed\n",
    "spark.sql(\"select country_code, date_format(window.end, 'MMM-dd HH:mm') as time, count from counts order by time, country_code\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, let's see the total number of \"opens\" and \"closes\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spark.sql(\"select country_code, sum(count) as total_count from counts group by country_code order by country_code\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check for running streams:\n",
    "# spark.streams.active\n",
    "\n",
    "#Terminate the query stream\n",
    "#query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# some stats for the query\n",
    "# print(query.lastProgress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "PySpark (Python 3, Spark 2.1.0)",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "name": "structured-streaming-python",
  "notebookId": 1458106420266562
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
