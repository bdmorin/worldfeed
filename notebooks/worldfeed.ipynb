{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## satori2Kafka "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import socket\n",
    "import json\n",
    "import sys\n",
    "import threading\n",
    "import time\n",
    "from satori.rtm.client import make_client, SubscriptionMode\n",
    "from kafka import KafkaProducer\n",
    "\n",
    "def satori2kafka(channel,endpoint, appkey):\n",
    "    # Kafka\n",
    "    producer = KafkaProducer(bootstrap_servers=['localhost:9092'])\n",
    "    topic = \"world-feed\"\n",
    "\n",
    "    with make_client(\n",
    "            endpoint=endpoint, appkey=appkey) as client:\n",
    "        print('Connected!')\n",
    "        mailbox = []\n",
    "        got_message_event = threading.Event()\n",
    "\n",
    "        class SubscriptionObserver(object):\n",
    "            def on_subscription_data(self, data):\n",
    "                for message in data['messages']:\n",
    "                    mailbox.append(message)\n",
    "                    got_message_event.set()\n",
    "\n",
    "        subscription_observer = SubscriptionObserver()\n",
    "        client.subscribe(\n",
    "            channel,\n",
    "            SubscriptionMode.SIMPLE,\n",
    "            subscription_observer)\n",
    "\n",
    "        if not got_message_event.wait(10):\n",
    "            print(\"Timeout while waiting for a message\")\n",
    "            sys.exit(1)\n",
    "\n",
    "        while True:\n",
    "            for message in mailbox:\n",
    "                msg = json.dumps(message, ensure_ascii=False)\n",
    "                producer.send(topic, msg.encode())\n",
    "                # do not send the messages to fast for development\n",
    "                time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## helper functions for data enrichment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import langid\n",
    "\n",
    "def get_language_from_text(text):\n",
    "    lang, prob = langid.classify(text)\n",
    "    return lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from geolite2 import geolite2\n",
    "import socket\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "def get_country_from_url(url):\n",
    "    try:\n",
    "        hostname = urlparse(url)[1]\n",
    "        ip = socket.gethostbyname(hostname)\n",
    "        result = geolite2.reader().get(ip)\n",
    "        country_iso_code = result['country']['iso_code']\n",
    "    except:\n",
    "        country_iso_code = \"unknown\"\n",
    "    finally:\n",
    "        geolite2.close()\n",
    "    return country_iso_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define the schema of the initial rss feed from satori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "# Since we know the data format already, let's define the schema to speed up processing (no need for Spark to infer schema)\n",
    "jsonSchema = StructType([ StructField(\"publishedTimestamp\", TimestampType(), True), \n",
    "                          StructField(\"url\", StringType(), True),\n",
    "                          StructField(\"feedURL\", StringType(), True),\n",
    "                          StructField(\"title\", StringType(), True),\n",
    "                          StructField(\"description\", StringType(), True)\n",
    "                        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stream Processing \n",
    "read from kafka to a streaming data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Similar to definition of staticInputDF above, just using `readStream` instead of `read`\n",
    "parsed = (\n",
    "  spark\n",
    "    .readStream                       \n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\n",
    "    .option(\"subscribe\", \"world-feed\")\n",
    "    .load()\n",
    "    .select(col(\"timestamp\"),from_json(col(\"value\").cast(\"string\"),jsonSchema).alias(\"parsed_value\"))\n",
    ")\n",
    "\n",
    "worldfeed = parsed.select(\"timestamp\",\"parsed_value.*\")\n",
    "worldfeed.printSchema()\n",
    "worldfeed.isStreaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### enrich the streaming data frame with language and country of origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf\n",
    "from worldfeed.location_lookup import get_country_from_url\n",
    "\n",
    "language_classify_udf = udf(get_language_from_text, StringType())\n",
    "get_country_from_url_udf = udf(get_country_from_url, StringType())\n",
    "\n",
    "enriched_df = (\n",
    "  worldfeed\n",
    "    .withColumn('language', language_classify_udf(worldfeed['description']))\n",
    "    .withColumn('server_country', get_country_from_url_udf(worldfeed['feedURL']))\n",
    ")\n",
    "enriched_df.isStreaming\n",
    "enriched_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## start the streaming\n",
    "aggregate the data and write back to kafka for further processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")  # keep the size of shuffles small\n",
    "query = (\n",
    "  enriched_df\n",
    "    .withWatermark(\"timestamp\", \"15 minutes\")\n",
    "    .groupBy(\n",
    "      enriched_df.server_country,\n",
    "      enriched_df.language, \n",
    "      window(enriched_df.timestamp, \"15 minutes\"))  \n",
    "    .count()\n",
    "    .select(to_json(struct(\"server_country\", \"window\")).alias(\"key\"),\n",
    "        to_json(struct(\"window.start\",\"window.end\",\"server_country\", \"language\", \"count\")).alias(\"value\"))\n",
    "    .writeStream\n",
    "    .trigger(processingTime='5 seconds')\n",
    "\n",
    "    # output to console for debug\n",
    "    # .format(\"console\")\n",
    "\n",
    "    # output to kafka \n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\n",
    "    .option(\"topic\", \"enriched-feed\")\n",
    "    .option(\"checkpointLocation\", \"./checkpoints\")\n",
    "    # End kafka related output\n",
    "    .outputMode(\"update\")  # complete = all the counts should be in the table\n",
    "    .queryName(\"worldfeed\")     # counts = name of the in-memory table\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the Satori 2 Kafka Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "channel = \"big-rss\"\n",
    "endpoint = \"wss://open-data.api.satori.com\"\n",
    "appkey = \"8e7f2BeFE8C8c6e8A4A41976a2dE5Fa9\"\n",
    "\n",
    "satori2kafka(channel, endpoint, appkey)\n",
    "# has to be manually cancelled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "PySpark (Python 3, Spark 2.1.1)",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "name": "structured-streaming-python",
  "notebookId": 1458106420266562
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
