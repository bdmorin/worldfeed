{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing origins and language of RSS feeds\n",
    "\n",
    "The idea is to consume the [big rss feed](https://www.satori.com/channels/big-rss) as a stream from [Satori](https://www.satori.com/), enrich each message with Spark Structured Streaming and visualize it in a browser app.\n",
    "\n",
    "The Satori stream gathers more than 6.5 million rss feeds. The stream is consumed as websocket with a python function and written into the kafka topic *world-feed*.\n",
    "Spark streaming is then used to read from the topic into a streaming dataframe which is enhanced with 2 informations:\n",
    "\n",
    "1. Country of the server the message is comming from\n",
    "2. The language of the message\n",
    "\n",
    "The stream aggregates the message count by a 15 minute time window, the country_code and the language. All updates are written every 5 seconds into a second kafka topic *enriched-feed*.\n",
    "\n",
    "The visualisation is done with a small node.js app consuming the kafka topic and sending the message via websockets to all connected browsers where a reactJS app is handling the update of charts.\n",
    "\n",
    "\n",
    "## System Architecture\n",
    "![System Architecture Data Streaming](SystemArchitectureDataStreaming.png?raw=true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## satori2Kafka \n",
    "a generic function consuming a satori stream identified by channel, endpoint and appkey and writing each message to a kafka topic. The message is in the JSON format.\n",
    "The stream is slowed down for development purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import socket\n",
    "import json\n",
    "import sys\n",
    "import threading\n",
    "import time\n",
    "from satori.rtm.client import make_client, SubscriptionMode\n",
    "from kafka import KafkaProducer\n",
    "\n",
    "def satori2kafka(channel,endpoint, appkey, topic, delay=1):\n",
    "    # Kafka\n",
    "    producer = KafkaProducer(bootstrap_servers=['localhost:9092'])\n",
    "    \n",
    "    with make_client(\n",
    "            endpoint=endpoint, appkey=appkey) as client:\n",
    "        print('Connected!')\n",
    "        mailbox = []\n",
    "        got_message_event = threading.Event()\n",
    "\n",
    "        class SubscriptionObserver(object):\n",
    "            def on_subscription_data(self, data):\n",
    "                for message in data['messages']:\n",
    "                    mailbox.append(message)\n",
    "                    got_message_event.set()\n",
    "\n",
    "        subscription_observer = SubscriptionObserver()\n",
    "        client.subscribe(\n",
    "            channel,\n",
    "            SubscriptionMode.SIMPLE,\n",
    "            subscription_observer)\n",
    "\n",
    "        if not got_message_event.wait(10):\n",
    "            print(\"Timeout while waiting for a message\")\n",
    "            sys.exit(1)\n",
    "\n",
    "        while True:\n",
    "            for message in mailbox:\n",
    "                msg = json.dumps(message, ensure_ascii=False)\n",
    "                producer.send(topic, msg.encode())\n",
    "                # do not send the messages to fast for development\n",
    "                time.sleep(delay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## helper functions for data enrichment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Identification\n",
    "simple usage of the python [langid](https://github.com/saffsd/langid.py) library to identify in which language the message is written. The function returns a 2 letter iso code for the identified language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import langid\n",
    "\n",
    "def get_language_from_text(text):\n",
    "    lang, prob = langid.classify(text)\n",
    "    return lang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Country Identification\n",
    "with the extracted hostname from the url the ip address can be fetched using a nameserver lookup on the local machine. The [geoip2](https://pypi.python.org/pypi/geoip2) library allows a lookup with the IP address in the [Maxmind](http://dev.maxmind.com/geoip/geoip2/geolite2/) geolocation database. The iso country code is returned by the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from geolite2 import geolite2\n",
    "import socket\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "def get_country_from_url(url):\n",
    "    try:\n",
    "        hostname = urlparse(url)[1]\n",
    "        ip = socket.gethostbyname(hostname)\n",
    "        result = geolite2.reader().get(ip)\n",
    "        country_iso_code = result['country']['iso_code']\n",
    "    except:\n",
    "        country_iso_code = \"unknown\"\n",
    "    finally:\n",
    "        geolite2.close()\n",
    "    return country_iso_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Structured Streaming\n",
    "\n",
    "### Read Stream\n",
    "with spark structured streaming we connect to a kafka topic and continuously append each message to a dataframe. Each kafka record consists of a **key**, a **value**, and a **timestamp**.\n",
    "the **value** contains our satori message in the JSON format. For further processing we apply the jsonSchema to the **value** field and create a new streaming dataframe where we keep the **timestamp** from the kafka record together with the satori structured message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Since we know the data format already, \n",
    "# let's define the schema to speed up processing \n",
    "# (no need for Spark to infer schema)\n",
    "jsonSchema = StructType([ StructField(\"publishedTimestamp\", TimestampType(), True), \n",
    "                          StructField(\"url\", StringType(), True),\n",
    "                          StructField(\"feedURL\", StringType(), True),\n",
    "                          StructField(\"title\", StringType(), True),\n",
    "                          StructField(\"description\", StringType(), True)\n",
    "                        ])\n",
    "# define 'parsed' as a structured stream from the \n",
    "# kafka records in the topic 'world-feed'.\n",
    "parsed = (\n",
    "  spark\n",
    "    .readStream                       \n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\n",
    "    .option(\"subscribe\", \"world-feed\")\n",
    "    .load()\n",
    "    # keep timestamp and the json from the field value in a new field 'parsed_value'\n",
    "    .select(col(\"timestamp\"),from_json(col(\"value\").cast(\"string\"),jsonSchema).alias(\"parsed_value\"))\n",
    ")\n",
    "# print the current schema \n",
    "parsed.printSchema()\n",
    "\n",
    "# get rid of the struct 'parsed_value' and keep only the fields beneath.\n",
    "worldfeed = parsed.select(\"timestamp\",\"parsed_value.*\")\n",
    "\n",
    "# print the schema which is used for further processing\n",
    "worldfeed.printSchema()\n",
    "\n",
    "# show that the dataframe is streaming\n",
    "worldfeed.isStreaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data enrichment\n",
    "the dataframe function withColumn() allows us to add a new column to a dataframe by applying a function to existing columns. For this, an existing function has to be converted to a **U**ser**D**efined**F**unction. This function can than be applied to a distributed dataframe.\n",
    "\n",
    "The *get_country_from_url()* functions is too big to be serialized. It is therefore loaded from a library.\n",
    "Be aware that any library used in such a function has to be made available on the worker nodes executing the job.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "# as get_country_from_url can not be serialized it is loaded from a library. \n",
    "# Therefore worldfeed.location_lookup has to be installed on all worker nodes.\n",
    "\n",
    "from worldfeed.location_lookup import get_country_from_url\n",
    "\n",
    "# transform the helpers to UDFs.\n",
    "language_classify_udf = udf(get_language_from_text, StringType())\n",
    "get_country_from_url_udf = udf(get_country_from_url, StringType())\n",
    "\n",
    "# create a new dataframe with the additional columns 'language' and 'server_country'\n",
    "enriched_df = (\n",
    "  worldfeed\n",
    "    .withColumn('language', language_classify_udf(worldfeed['description']))\n",
    "    .withColumn('server_country', get_country_from_url_udf(worldfeed['feedURL']))\n",
    ")\n",
    "\n",
    "# print the new schema\n",
    "enriched_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## start the streaming query\n",
    "\n",
    "based on the enriched_df dataframe, a query is written which aggregates the data, reformats it into a kafka readable format and writes it to a kafka topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")  # keep the size of shuffles small\n",
    "\n",
    "query = (\n",
    "  enriched_df\n",
    "    # watermarking allows to update passed timeframes with late arrivals\n",
    "    # after 30 minutes the timeframe is frozen and can be removed from memory\n",
    "    .withWatermark(\"timestamp\", \"30 minutes\")\n",
    "    \n",
    "    # aggregation happens by server_country, the language and a 15 minute timeframe\n",
    "    .groupBy(\n",
    "      enriched_df.server_country,\n",
    "      enriched_df.language, \n",
    "      window(enriched_df.timestamp, \"15 minutes\"))\n",
    "    # the messages are counted\n",
    "    .count()\n",
    "    \n",
    "    # the query result is written to a kafka topic, \n",
    "    #therefore the output has to consist of a 'key' and a 'value'\n",
    "    # key: \n",
    "    .select(to_json(struct(\"server_country\", \"window\")).alias(\"key\"),\n",
    "    # value: (json format the mentioned fields)\n",
    "        to_json(struct(\"window.start\",\"window.end\",\"server_country\", \"language\", \"count\")).alias(\"value\"))\n",
    "    .writeStream\n",
    "    # only write every 5 seconds\n",
    "    .trigger(processingTime='5 seconds')\n",
    "\n",
    "    # output to console for debug\n",
    "    # .format(\"console\")\n",
    "\n",
    "    # output to kafka \n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\n",
    "    .option(\"topic\", \"enriched-feed\")\n",
    "    .option(\"checkpointLocation\", \"./checkpoints\")\n",
    "    # End kafka related output\n",
    "    # only write the rows that where updated since the last update\n",
    "    .outputMode(\"update\") \n",
    "    .queryName(\"worldfeed\")\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## start the satori2kafka stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "channel = \"big-rss\"\n",
    "endpoint = \"wss://open-data.api.satori.com\"\n",
    "appkey = \"8e7f2BeFE8C8c6e8A4A41976a2dE5Fa9\"\n",
    "topic = \"world-feed\"\n",
    "\n",
    "satori2kafka(channel, endpoint, appkey, topic)\n",
    "# has to be manually cancelled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## start the node.js app\n",
    "make sure to start the node.js app in visualize_world_feed\n",
    "command: node index.js\n",
    "\n",
    "the point your browser to http://localhost:3001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# helpers\n",
    "check if there are any active streaming queries. query.stop() terminates the query. Only 1 query of the same instance can run simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spark.streams.active\n",
    "query.stop()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "PySpark (Python 3, Spark 2.1.1)",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "name": "structured-streaming-python",
  "notebookId": 1458106420266562
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
