{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "import json\n",
    "from pyspark.sql.functions import *\n",
    "# Since we know the data format already, let's define the schema to speed up processing (no need for Spark to infer schema)\n",
    "#jsonSchema = StructType([StructField(\"name\", StringType(), True) ])\n",
    "jsonSchema = StructType([ StructField(\"last_reported\", TimestampType(), True), StructField(\"name\", StringType(), True) ])\n",
    "\n",
    "# Strem DataFrame representing data in the JSON files\n",
    "lines = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"socket\") \\\n",
    "    .option(\"host\", \"localhost\") \\\n",
    "    .option(\"port\", 9985) \\\n",
    "    .schema(jsonSchema) \\\n",
    "    .load()\n",
    "\n",
    "stream_objects = lines.select(from_json(lines.value, jsonSchema).alias(\"stream_object\"))\n",
    "stream_objects.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")  # keep the size of shuffles small\n",
    "\n",
    "query = (\n",
    "    stream_objects\n",
    "    .writeStream\n",
    "    .format(\"memory\")        # memory = store in-memory table (for testing only in Spark 2.0)\n",
    "    .queryName(\"counts\")     # counts = name of the in-memory table\n",
    "    .outputMode(\"append\")  # complete = all the counts should be in the table\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#spark.sql(\"select * from counts\").toJSON().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# RUN a simple webservice on default port 5000 to return the in-memory table as is\n",
    "from flask import Flask\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# route to access data\n",
    "@app.route(\"/\")\n",
    "def hello():\n",
    "        rdd_json = spark.sql(\"select * from counts\").toJSON().collect()\n",
    "    return json.dumps(rdd_json)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "PySpark (Python 3, Spark 2.1.0)",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "name": "structured-streaming-python",
  "notebookId": 1458106420266562
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
